DataParams

dataset_name = "wikitext"
debug_crop = 100

TrainParams

batch_size = 32
accumulate_grad_batches = 1
epochs = 1
lr = 3e-4
max_sample_tokens = 20
lr_warm_steps = 100
lr_cycle_steps = 500
val_check_epoch_frac = 0.1

ModelParams

n_layers = 4
n_heads = 4
dim = 256
max_seq_len = 100
tokenizer_source_name = "gpt2"